# -*- coding: utf-8 -*-
"""Rezwan_Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_P9hztlA81K48eIr7iG__Rvab6wc-Pi6
"""

import os
from skimage import io
from natsort import natsorted,ns
from skimage.transform import resize
from skimage.color import rgb2gray
from matplotlib import pyplot as plt
import numpy as np

"""# Loading m image and coverting RGB image to grayscale"""

path ="E:\\EnglishImg\\English\\Img\\GoodImg\\Bmp\\Sample049"
list_files = os.listdir(path)
print(list_files)
list_files = natsorted(list_files)
print(list_files)
image_list1= []
m_image_list1=[]
for filename in list_files:
    image_list1.append(rgb2gray(io.imread(os.path.join(path,filename))))
print(plt.imshow(image_list1[0]))

"""# Resized m to 28*28"""

resized_image_list1=[]

for image in image_list1:
    image = resize(image, (28, 28))
    resized_image_list1.append(image)
print(resized_image_list1[0].shape)

"""# Reshape 'm' IMAGE FROM (784, ) to (1, 784) and making an array of (m,pixels)"""

flatten_list1 = resized_image_list1[0].reshape((1,784))
print(flatten_list1.shape)
for idx, img in enumerate(resized_image_list1):
    if(idx != 0):
        flatten_list1 = np.concatenate((flatten_list1,img.reshape((1,784))))
print(flatten_list1.shape)

"""## RGB image for section 1.4"""

m_image_list1=[]
for filename in list_files:
    m_image_list1.append(io.imread(os.path.join(path,filename)))
print(plt.imshow(m_image_list1[0]))

m_resized_image_list1=[]

for image in m_image_list1:
    image = resize(image, (28, 28,28))
    m_resized_image_list1.append(image)
print(m_resized_image_list1[0].shape)
m_flatten_list1 = m_resized_image_list1[0].reshape((1,21952))
print(m_flatten_list1.shape)
for idx, img in enumerate(m_resized_image_list1):
    if(idx != 0):
        m_flatten_list1 = np.concatenate((m_flatten_list1,img.reshape((1,21952))))
print(m_flatten_list1.shape)

"""# Loading n image and coverting RGB image to grayscale"""

path ="E:\\EnglishImg\\English\\Img\\GoodImg\\Bmp\\Sample050"
list_files = os.listdir(path)
print(list_files)
list_files = natsorted(list_files)
print(list_files)
image_list2 = []
for filename in list_files:
    image_list2.append(rgb2gray(io.imread(os.path.join(path,filename))))
plt.imshow(image_list2[0])

"""# Resized n to 28*28  

"""

resized_image_list2=[]
for image in image_list2:
    image = resize(image, (28, 28))
    resized_image_list2.append(image)
print(resized_image_list2[0].shape)

"""# Reshape 'n' IMAGE FROM (784, ) to (1, 784) and making an array of (n,pixels)"""

flatten_list2 = resized_image_list2[0].reshape((1,784))
print(flatten_list2.shape)
for idx, img in enumerate(resized_image_list2):
    if(idx != 0):
        flatten_list2 = np.concatenate((flatten_list2,img.reshape((1,784))))
print(flatten_list2.shape)

"""## RGB image for section 1.4"""

n_image_list2=[]
for filename in list_files:
    n_image_list2.append(io.imread(os.path.join(path,filename)))
print(plt.imshow(n_image_list2[0]))

n_resized_image_list2=[]

for image in n_image_list2:
    image = resize(image, (28, 28,28))
    n_resized_image_list2.append(image)
print(n_resized_image_list2[0].shape)
n_flatten_list2 = n_resized_image_list2[0].reshape((1,21952))
print(n_flatten_list2.shape)
for idx, img in enumerate(n_resized_image_list2):
    if(idx != 0):
        n_flatten_list2 = np.concatenate((n_flatten_list2,img.reshape((1,21952))))
print(n_flatten_list2.shape)

"""# Loading s image and coverting RGB image to grayscale"""

path ="E:\\EnglishImg\\English\\Img\\GoodImg\\Bmp\\Sample055"
list_files = os.listdir(path)
print(list_files)
list_files = natsorted(list_files)
print(list_files)
image_list3 = []
for filename in list_files:
    image_list3.append(rgb2gray(io.imread(os.path.join(path,filename))))
plt.imshow(image_list3[0])

"""# Resized s to 28*28"""

resized_image_list3=[]
for image in image_list3:
    image = resize(image, (28, 28))
    resized_image_list3.append(image)
print(resized_image_list3[0].shape)

"""# Reshape 's' IMAGE FROM (784, ) to (1, 784) and making an array of (s,pixels)"""

flatten_list3 = resized_image_list3[0].reshape((1,784))
print(flatten_list3.shape)
for idx, img in enumerate(resized_image_list3):
    if(idx != 0):
        flatten_list3= np.concatenate((flatten_list3,img.reshape((1,784))))
print(flatten_list3.shape)

"""## RGB image for section 1.4"""

s_image_list3=[]
for filename in list_files:
    s_image_list3.append(io.imread(os.path.join(path,filename)))
print(plt.imshow(s_image_list3[0]))

s_resized_image_list3=[]

for image in s_image_list3:
    image = resize(image, (28, 28,28))
    s_resized_image_list3.append(image)
print(s_resized_image_list3[0].shape)
s_flatten_list3 = s_resized_image_list3[0].reshape((1,21952))
print(s_flatten_list3.shape)
for idx, img in enumerate(s_resized_image_list3):
    if(idx != 0):
        s_flatten_list3 = np.concatenate((s_flatten_list3,img.reshape((1,21952))))
print(s_flatten_list3.shape)

"""# # Loading 5's image and coverting RGB image to grayscale"""

path ="E:\\EnglishImg\\English\\Img\\GoodImg\\Bmp\\Sample006"
list_files = os.listdir(path)
print(list_files)
list_files = natsorted(list_files)
print(list_files)
image_list4 = []
for filename in list_files:
    image_list4.append(rgb2gray(io.imread(os.path.join(path,filename))))
plt.imshow(image_list4[0])

"""# Resized 5 to 28*28"""

resized_image_list4=[]
for image in image_list4:
    image = resize(image, (28, 28))
    resized_image_list4.append(image)
print(resized_image_list4[0].shape)

"""# Reshape '5' IMAGE FROM (784, ) to (1, 784) and making an array of (5,pixels)"""

flatten_list4 = resized_image_list4[0].reshape((1,784))
print(flatten_list4.shape)
for idx, img in enumerate(resized_image_list4):
    if(idx != 0):
        flatten_list4= np.concatenate((flatten_list4,img.reshape((1,784))))
print(flatten_list4.shape)

"""## RGB image for section 1.4"""

image_5_list4=[]
for filename in list_files:
    image_5_list4.append(io.imread(os.path.join(path,filename)))
print(plt.imshow(image_5_list4[0]))

resized_image_5_list4=[]

for image in image_5_list4:
    image = resize(image, (28, 28,28))
    resized_image_5_list4.append(image)
print(resized_image_5_list4[0].shape)
flatten_5_list4 = resized_image_5_list4[0].reshape((1,21952))
print(flatten_5_list4.shape)
for idx, img in enumerate(resized_image_5_list4):
    if(idx != 0):
        flatten_5_list4 = np.concatenate((flatten_5_list4,img.reshape((1,21952))))
print(flatten_5_list4.shape)

"""# 1.1 BINARY CLASSIFIER : m or n

Analysis of the result:

1.I have used np.random.permutation() function random image selection splitting and reshuffle the data and use 85% samples as  training and 15% as test

2.I have used both K-fold and stratified k fold for Stratified Sampling and found the mean score and mean accuracy of each   model.

3.Random splitting gives better results than stratified sampling. It should have given better results when we use any cross validation method instead of selecting images randomly but due to low data points, each class contains less data points which make it hard for k-Fold to give a better prediction.

4.Analysis of  Confusion matrix, the accuracy, the recall and the precision are given below

5.Analysis Roc Curves is given below

## Creating Target label array
"""

m_target=np.zeros(len(flatten_list1))
n_target=np.ones(len(flatten_list2))

combined_targetlist=np.concatenate((m_target,n_target))
print(combined_targetlist[177])
print(combined_targetlist.shape)

"""## Combining all m and n images together in an np array."""

combined_m_n_image=np.concatenate((flatten_list1,flatten_list2))
print(combined_m_n_image[1][783])
print(combined_m_n_image.shape)

"""## Creating DataFrame"""

import pandas as pd
df1=pd.DataFrame(combined_m_n_image)
df2=pd.DataFrame(combined_targetlist,columns=['Target'])
print(df1.head())
print(df2.head())

"""## Concatenating image dataframe with target dataframe"""

df3 = pd.concat([df1, df2], axis=1)

print(df3['Target'])

# Use the notion of y=f(X)
X, y = df3.iloc[:,0:784], df3["Target"]
print('Data size: {0} x {1} and label size {2}'.format(X.shape[0],X.shape[1],y.shape[0]))

import math
print('The images are of size: {0} x {0}'.format(math.sqrt(X.shape[1])))

"""## Converting Target Column's dataType from float to int"""

y = y.astype(np.int32)
print(y.head())

"""## Random data splitting"""

[shuffle_index[151:]]

"""## Building and Evaluating the Model:"""

from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import LogisticRegression

classifier= LogisticRegression(max_iter=10, tol=0.1, solver='liblinear')
# sgd_clf = SGDClassifier(max_iter=5, random_state=42)
classifier.fit(X_train, y_train)

"""## Making Prediction on Training Set"""

from sklearn.metrics import accuracy_score

y_pred = classifier.predict(X_train)
print('Classifier accuracy on the training set is {0} '.format(accuracy_score(y_train, y_pred)))

"""## Confusion matrix, the accuracy, the recall and the precision

### Analysis

Predicting 0 mean m and  Predicting 1 means n

The data is im-balance as n samples are 6 times greater than m samples, so accuracy should not be the standard metric for this model.
The sample number of n is greater than m, so TP>TN
Precision:

Model was able to predict  116 images of n correctly and it was actually n
Recall :

Model was able to predict  116 images of n correctly  from actually positive images
F1 Score :

For this model F1 score is the best metric due to imbalanced class distribution.

Training  error <  Test error , so it has Low Bias and High Variance. So it is a case of overfitting.
"""

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

print('Classifier scores on training set: ')
print('Precision is {0} '.format(precision_score(y_train, y_pred)))
print('Recall is {0} '.format(recall_score(y_train, y_pred)))
print('F1 is {0} '.format(f1_score(y_train, y_pred)))
cf_matrix=confusion_matrix(y_train, y_pred)
print( 'And the confusion matrix: \n {0}'.format(cf_matrix) )

"""### Manually Analysing the Accuracy,Precison ,Recall and F1 score on training set"""

Accuuracy=(116+20)/(116+20+15+0)
Precison =116/(116+15)
Recall =116/(116+0)
F1=2*((Precison*Recall)/(Precison+Recall))
print(Accuuracy)
print(Precison)
print(Recall)
print(F1)

"""## Making Prediction on Test Set"""

y_pred = classifier.predict(X_test)
print('Classifier accuracy on the test set is {0} '.format(accuracy_score(y_test, y_pred)))

"""## Confusion matrix, the accuracy, the recall and the precision of  TEST SET"""

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

print('Classifier scores on test set: ')
print('Precision is {0} '.format(precision_score(y_test, y_pred)))
print('Recall is {0} '.format(recall_score(y_test, y_pred)))
print('F1 is {0} '.format(f1_score(y_test, y_pred)))
cf_matrix2=confusion_matrix(y_test, y_pred)
print( 'And the confusion matrix: \n {0}'.format(cf_matrix2) )

"""## Random vs some form of stratified sampling
### 1. 10 K-fold
### 2. Stratified_10K_Fold
"""

from sklearn.model_selection import cross_val_score
score=cross_val_score(classifier, X_train, y_train, cv=10, scoring="accuracy")

print(score)
print(score.mean())

from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

accuracy =[]
skf = StratifiedKFold(n_splits=10)
skf.get_n_splits(X_train,y_train)
# We could random shuffle but than the reulst will not be exactly the same
# skfolds = StratifiedKFold(n_splits=5), shuffle=True, random_state=42)

for train_index, test_index in skf.split(X_train, y_train):

    X_train_folds = X_train.iloc[train_index]
    y_train_folds = y_train.iloc[train_index]
    X_test_fold = X_train.iloc[test_index]
    y_test_fold = y_train.iloc[test_index]

    classifier.fit(X_train_folds, y_train_folds)
    y_pred = classifier.predict(X_test_fold)
    score=accuracy_score(y_pred,y_test_fold)
    accuracy.append(score)

print(accuracy)
print(np.array(accuracy).mean())

"""### Roc analysis
This Roc curve here is not very ideal. Are under curve is less than 0.5 and it has no capacity to distinguish between positive class and negative class.
"""

from sklearn.model_selection import cross_val_predict

y_scores = cross_val_predict(classifier, X_train, y_train, cv=5,
                             method="decision_function")

from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_train, y_scores)

def plot_roc_curve(fpr, tpr, label=None):
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.axis([0, 1, 0, 1])
    plt.xlabel('False Positive Rate', fontsize=16)
    plt.ylabel('True Positive Rate', fontsize=16)

plt.figure(figsize=(8, 6))
plot_roc_curve(fpr, tpr)
plt.show()

"""# 1.2 BINARY CLASSIFIER : m or S

## Creating Target label array
"""

m_target=np.zeros(len(flatten_list1))
s_target=np.ones(len(flatten_list3))

combined_targetlist=np.concatenate((m_target,s_target))
print(combined_targetlist[150])
print(combined_targetlist.shape)

"""## Combining all m and n images together in an np array."""

combined_m_s_image=np.concatenate((flatten_list1,flatten_list3))
print(combined_m_s_image[1][783])
print(combined_m_s_image.shape)

"""## Creating DataFrame"""

import pandas as pd
df1=pd.DataFrame(combined_m_s_image)
df2=pd.DataFrame(combined_targetlist,columns=['Target'])
print(df1.head())
print(df2.head())

"""## Concatenating image dataframe with target dataframe"""

df3 = pd.concat([df1, df2], axis=1)
print(df3.shape)

# Use the notion of y=f(X)
X, y = df3.iloc[:,0:784], df3["Target"]
print('Data size: {0} x {1} and label size {2}'.format(X.shape[0],X.shape[1],y.shape[0]))

"""## Converting Target Column's dataType from float to int"""

y = y.astype(np.int32)
print(y.head())

"""## Random data splitting using train_test_split"""

shuffle_index = np.random.permutation(167)

X_train, X_test, y_train, y_test = X.iloc[shuffle_index[:141],:],X.iloc[shuffle_index[141:],:],y.iloc[shuffle_index[:141]], y.iloc[shuffle_index[141:]]

"""## Building and Evaluating the Model:"""

from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import LogisticRegression

classifier= LogisticRegression(max_iter=10, tol=0.1, solver='liblinear')
# sgd_clf = SGDClassifier(max_iter=5, random_state=42)
classifier.fit(X_train, y_train)

"""## Making Prediction on Training Set"""

from sklearn.metrics import accuracy_score

y_pred = classifier.predict(X_train)
print('Classifier accuracy on the training set is {0} '.format(accuracy_score(y_train, y_pred)))

"""### Analysis of the result :

Predicting 0 mean m and  Predicting 1 means s

The data is im-balance as n samples are 4.5 times greater than m samples, so accuracy should not be the standard metric for this model.
The sample number of s is greater than m, so TP>TN
Precision:

Model was able to predict  108 images of s correctly and it was actually s
Recall :

Model was able to predict  108 images of s correctly  from actually positive images
F1 Score :

For this model F1 score is the best metric due to imbalanced class distribution.

Training  error <  Test error , so it has Low Bias and High Variance. So it is a case of overfitting.

## Confusion matrix, the accuracy, the recall and the precision of Training Set
"""

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

print('Classifier scores on training set: ')
print('Precision is {0} '.format(precision_score(y_train, y_pred)))
print('Recall is {0} '.format(recall_score(y_train, y_pred)))
print('F1 is {0} '.format(f1_score(y_train, y_pred)))

print( 'And the confusion matrix: \n {0}'.format(confusion_matrix(y_train, y_pred) ))

Accuuracy=(108+24)/(108+24+9+0)
Precison =108/(108+9)
Recall= 108/(108+0)
F1=2*((Precison*Recall)/(Precison+Recall))
print(Accuuracy)
print(Precison)
print(Recall)
print(F1)

"""## Making Prediction on Test Set"""

from sklearn.metrics import accuracy_score

y_pred = classifier.predict(X_test)
print('Classifier accuracy on the training set is {0} '.format(accuracy_score(y_test, y_pred)))

"""## Confusion matrix, the accuracy, the recall and the precision of Test Set"""

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

print('Classifier scores on training set: ')
print('Precision is {0} '.format(precision_score(y_test, y_pred)))
print('Recall is {0} '.format(recall_score(y_test, y_pred)))
print('F1 is {0} '.format(f1_score(y_test, y_pred)))

print( 'And the confusion matrix: \n {0}'.format(confusion_matrix(y_test, y_pred) ))

"""## random vs some form of stratified sampling
### 1. 10 K-fold
### 2. Stratified_10K_Fold
"""

from sklearn.model_selection import cross_val_score
score=cross_val_score(classifier, X_train, y_train, cv=10, scoring="accuracy")

print(score)
print(score.mean())

from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

accuracy =[]
skf = StratifiedKFold(n_splits=10)
skf.get_n_splits(X_train,y_train)
# We could random shuffle but than the reulst will not be exactly the same
# skfolds = StratifiedKFold(n_splits=5), shuffle=True, random_state=42)

for train_index, test_index in skf.split(X_train, y_train):

    X_train_folds = X_train.iloc[train_index]
    y_train_folds = y_train.iloc[train_index]
    X_test_fold = X_train.iloc[test_index]
    y_test_fold = y_train.iloc[test_index]

    classifier.fit(X_train_folds, y_train_folds)
    y_pred = classifier.predict(X_test_fold)
    score=accuracy_score(y_pred,y_test_fold)
    accuracy.append(score)

print(accuracy)
print(np.array(accuracy).mean())

"""### Roc analysis
This Roc curve here is not perfecly ideal not it is very bad.. Are under curve is less than 1. This is beacuse we have certain error in the prediction. Basically there are  some FP and FN for which are under curve is a bit less than 1.
"""

from sklearn.model_selection import cross_val_predict

y_scores = cross_val_predict(classifier, X_train, y_train, cv=5,
                             method="decision_function")

from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_train, y_scores)

def plot_roc_curve(fpr, tpr, label=None):
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.axis([0, 1, 0, 1])
    plt.xlabel('False Positive Rate', fontsize=16)
    plt.ylabel('True Positive Rate', fontsize=16)

plt.figure(figsize=(8, 6))
plot_roc_curve(fpr, tpr)
plt.show()

"""## Data Visualisation

# 1.3 Multiclass Classifer : m or s or n or 5

### Analysis of the result:

1.I have used train_test_split for random splitting (75% training set and 25 % test set)

2.I have used both K-fold and stratified k-fold for Stratified Sampling and found the mean score and mean accuracy of each model.

4.Random splitting gives better results than stratified sampling. It should have give better result when we use any cross validation method instead of selecting image randomly but due to low data points, each class contain less data points which make it hard for k-Fold to give a better prediction.

5.Analysis of Accuracy and Confusion Matrix are given below:

## Creating Target label array
"""

m = np.zeros(len(flatten_list1),dtype=np.int)
n = np.ones(len(flatten_list2),dtype=np.int)
s = np.ones(len(flatten_list3),dtype=np.int)*2
img_5 =np.ones(len(flatten_list4), dtype=np.int)*3

print(m.shape)
print(n.shape)
print(s.shape)
print(img_5.shape)


targetlist_m_n_s_img_5=np.concatenate((m,n,s,img_5))
print(targetlist_m_n_s_img_5[365])
print(targetlist_m_n_s_img_5.shape)

"""## Combining all m and n images together in an np array."""

m_n_s_img_5=np.concatenate((flatten_list1,flatten_list2,flatten_list3,flatten_list4))
print(m_n_s_img_5[1][783])
print(m_n_s_img_5.shape)

"""## Creating DataFrame"""

import pandas as pd
df1=pd.DataFrame(m_n_s_img_5)
df2=pd.DataFrame(targetlist_m_n_s_img_5,columns=['Target'])
print(df1.head())
print(df2.head(366))

"""## Concatenating image dataframe with target dataframe"""

df3 = pd.concat([df1, df2], axis=1)
print(df3.shape)

print(df3.columns)

# Use the notion of y=f(X)
X, y = df3.iloc[:,0:784], df3["Target"]
print('Data size: {0} x {1} and label size {2}'.format(X.shape[0],X.shape[1],y.shape[0]))

"""## Random data splitting using train_test_split function"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=100)

"""## Building and Evaluating the Model:"""

from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import LogisticRegression

classifier= SGDClassifier(random_state=42)
# sgd_clf = SGDClassifier(max_iter=5, random_state=42)
classifier.fit(X_train, y_train)

"""### Analysis of Accuracy and Confusion Matrix
Predicting 0 means m and  Predicting 1 means in Predicting 2 means s Predicting 3 means '5'

The data is im-balance as n and s samples are way greater than m and '5' samples, so accuracy should not be the standard metric for this model.

Classifier were unable to give good accuracy due to m and 5 classes . This is because classifiers were more biased to n and s samples as there were more data samples to train on. So when predicting, the accuracy decreases as it was wrongly predicting m and 5 as less data was present to train this image more accurately.

Training error > 25 % and Test error >25 % , so it has High Bias and High Variance . It's a case of Underfitting,so we need more training data for this model.

## Accuracy and Confusion Matrix of Training Set
"""

from sklearn.metrics import accuracy_score

y_pred = classifier.predict(X_train)
print('Classifier accuracy on the training set is {0} '.format(accuracy_score(y_train, y_pred)))

y_train_pred = cross_val_predict(classifier, X_train, y_train, cv=5)
conf_mx = confusion_matrix(y_train, y_train_pred)
print('Confusion matrix: \n{0}'.format(conf_mx))

"""## Accuracy and Confusion Matrix of Test Set"""

from sklearn.metrics import accuracy_score

y_pred = classifier.predict(X_test)
print('Classifier accuracy on the test set is {0} '.format(accuracy_score(y_test, y_pred)))

y_test_pred = cross_val_predict(classifier, X_test, y_test, cv=5)
conf_mx = confusion_matrix(y_test, y_test_pred)
print('Confusion matrix: \n{0}'.format(conf_mx))

"""## random vs some form of stratified sampling
### 1. 10 K-fold
### 2. Stratified_10 K_Fold
"""

from sklearn.model_selection import cross_val_score
score=cross_val_score(classifier, X_train, y_train, cv=5, scoring="accuracy")

print(score)
print(score.mean())

from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

accuracy =[]
skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train,y_train)

for train_index, test_index in skf.split(X_train, y_train):

    X_train_folds = X_train.iloc[train_index]
    y_train_folds = y_train.iloc[train_index]
    X_test_fold = X_train.iloc[test_index]
    y_test_fold = y_train.iloc[test_index]

    classifier.fit(X_train_folds, y_train_folds)
    y_pred = classifier.predict(X_test_fold)
    score=accuracy_score(y_pred,y_test_fold)
    accuracy.append(score)

print(accuracy)
print(np.array(accuracy).mean())

"""# 1.4 Improved Classifcation based on Improved Input Images

### Analysis :

1.By performing multiclass regression with RGB images instead of grayscale images, the training accuracy increases from 0.68 to 0.82. The training accuracy improved by 20 percent. However there was negligible improvement in test accuracy. So just using RGB images without improved image processing techniques will not improve this model.

2.I have used train_test_split for random splitting (75% training set and 25 % test set)

3.I have used both K-fold and stratified k-fold for Stratified Sampling and found the mean score and mean accuracy of each model.

4.Random splitting give better result than stratified sampling. It should have give better result when we use any cross validation method instead of selecting image randomly but due to low data pints, each class contain less data points which make it hard for k-Fold to give a better prediction.

### Analysis of Accuracy and Confusion Matrix
Predicting 0 means m and Predicting 1 means in Predicting 2 means s Predicting 3 means '5'
The data is im-balance as n and s samples are way greater than m and '5' samples, so accuracy should not be the standard metric for this model.
Classifier were unable to give good accuracy due to m and 5 classes . This is because classifiers were more biased to n and s samples as there were more data samples to train on. So when predicting, the accuracy decreases as it was wrongly predicting m and 5 as less data was present to train this image more accurately.
Training error < 25 % and Test error >25 % , so it has Low Bias and High Variance . It's a case of Overfitting ,so I can use regularization to minimize it.
Suggestions to improve the accuracy : To improve this accuracy , we should work with images with greater pixel, use RGB images as it reduces training error. Beside this we can use i binarizing ot morphology image processing technique to improve this model.
"""

m = np.zeros(len(m_flatten_list1),dtype=np.int)
n = np.ones(len(n_flatten_list2),dtype=np.int)
s = np.ones(len(s_flatten_list3),dtype=np.int)*2
img_5 =np.ones(len(flatten_5_list4), dtype=np.int)*3

print(m.shape)
print(n.shape)
print(s.shape)
print(img_5.shape)


targetlist_m_n_s_img_5=np.concatenate((m,n,s,img_5))
print(targetlist_m_n_s_img_5[365])
print(targetlist_m_n_s_img_5.shape)

m_n_s_img_5=np.concatenate((m_flatten_list1,n_flatten_list2,s_flatten_list3,flatten_5_list4))
print(m_n_s_img_5[1][783])
print(m_n_s_img_5.shape)

import pandas as pd
df1=pd.DataFrame(m_n_s_img_5)
df2=pd.DataFrame(targetlist_m_n_s_img_5,columns=['Target'])
print(df1.head())
print(df2.head())

df3 = pd.concat([df1, df2], axis=1)
print(df3.shape)

# Use the notion of y=f(X)
X, y = df3.iloc[:,0:21952], df3["Target"]
print('Data size: {0} x {1} and label size {2}'.format(X.shape[0],X.shape[1],y.shape[0]))

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=100)

from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import LogisticRegression

classifier= SGDClassifier(random_state=42)
# sgd_clf = SGDClassifier(max_iter=5, random_state=42)
classifier.fit(X_train, y_train)


y_pred = classifier.predict(X_train)
print('Classifier accuracy on the training set is {0} '.format(accuracy_score(y_train, y_pred)))

y_train_pred = cross_val_predict(classifier, X_train, y_train, cv=5)
conf_mx = confusion_matrix(y_train, y_train_pred)
print('Confusion matrix: \n{0}'.format(conf_mx))





accuracy =[]
skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train,y_train)

for train_index, test_index in skf.split(X_train, y_train):

    X_train_folds = X_train.iloc[train_index]
    y_train_folds = y_train.iloc[train_index]
    X_test_fold = X_train.iloc[test_index]
    y_test_fold = y_train.iloc[test_index]

    classifier.fit(X_train_folds, y_train_folds)
    y_pred = classifier.predict(X_test_fold)
    score=accuracy_score(y_pred,y_test_fold)
    accuracy.append(score)
print(accuracy)
print(np.array(accuracy).mean())

y_pred = classifier.predict(X_test)
print('Classifier accuracy on the test set is {0} '.format(accuracy_score(y_test, y_pred)))

y_test_pred = cross_val_predict(classifier, X_test, y_test, cv=5)
conf_mx = confusion_matrix(y_test, y_test_pred)
print('Confusion matrix: \n{0}'.format(conf_mx))